{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "## Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "! pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = 'snowflake/snowflake-arctic-embed-m-long'\n",
    "model_full_name = 'sentence-transformers/all-mpnet-base-v2'\n",
    "model_name = 'all-mpnet-base-v2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer (must match the model)\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_full_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\n",
    "    model_name, \n",
    "    trust_remote_code=True,\n",
    "    cache_folder=\"./models\"   # explicitly setting cache location\n",
    ")\n",
    "emb_dimensions = model.get_sentence_embedding_dimension()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"bfp-a3447q.pdf\"\n",
    "input_path = \"data/\" + filename.split('.')[0]+'.txt'\n",
    "output_path= input_path.split('.')[0]+'_v2_chunked.txt'\n",
    "image_path = \"data/images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, chunk in enumerate(data):\n",
    "    text = chunk[-1]\n",
    "    all_tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "    print(text)\n",
    "    print(all_tokens)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def chunk_text_by_lines(text, tokenizer, token_limit):\n",
    "    \"\"\"\n",
    "    Splits text into chunks based on token count, ensuring cuts only happen\n",
    "    between lines (preserving \\n).\n",
    "    \"\"\"\n",
    "    lines = text.splitlines(keepends=True)  # keep '\\n' at end of each line\n",
    "    chunks, current_chunk, current_tokens = [], [], 0\n",
    "    all_tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "    # setting 'soft limit'\n",
    "    chunk_size = math.ceil(len(all_tokens) / math.ceil(len(all_tokens)/ token_limit))\n",
    "    for line in lines:\n",
    "        line_tokens = tokenizer.encode(line, add_special_tokens=False)\n",
    "        if current_tokens + len(line_tokens) > token_limit:\n",
    "            # flush current chunk\n",
    "            chunks.append(\"\".join(current_chunk).strip())\n",
    "            current_chunk, current_tokens = [], 0\n",
    "        current_chunk.append(line)\n",
    "        current_tokens += len(line_tokens)\n",
    "        if current_tokens > chunk_size:\n",
    "            # flush current chunk\n",
    "            chunks.append(\"\".join(current_chunk).strip())\n",
    "            current_chunk, current_tokens = [], 0\n",
    "    \n",
    "    # add last chunk\n",
    "    if current_chunk:\n",
    "        chunks.append(\"\".join(current_chunk).strip())\n",
    "    \n",
    "    return chunks, chunk_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pathlib, json\n",
    "json_read = pathlib.Path(input_path).read_text()\n",
    "data = json.loads(json_read)\n",
    "chunked_data = []\n",
    "chunks = []\n",
    "token_limit = 300\n",
    "print(f'Token threshold: {token_limit}')\n",
    "for index, chunk in enumerate(data):\n",
    "    text = chunk[-1]\n",
    "    # Replace <br> with semicolons or newlines to reduce token consumption and clean tables\n",
    "    text = re.sub(r\"-<br\\s*/?>\", \"\", text) # merge splitted words\n",
    "    text = re.sub(r\"<br\\s*/?>\", \"; \", text) # change line separator to save some tokens\n",
    "    \n",
    "    # Strip off excessive whitespace\n",
    "    text = re.sub(r\" +\", \" \", text)\n",
    "    text = re.sub(r\"\\.+\", \".\", text)\n",
    "    text = re.sub(r\"~\", \"\", text)\n",
    "        \n",
    "    chunks, size = chunk_text_by_lines(text, tokenizer, token_limit)\n",
    "    for text_chunk in chunks:\n",
    "        chunked_data.append(\n",
    "            [chunk[0],\n",
    "             chunk[1],\n",
    "             chunk[2],\n",
    "             text_chunk]\n",
    "        )\n",
    "    if len(chunks) > 1:\n",
    "        print(f\"{index}) Text length: {len(text)}\\tToken chunk size: {size}\\tChunks count: {len(chunks)}\\t for {chunk[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(chunked_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "json_file=json.dumps(chunked_data, indent=2)\n",
    "# Exporting data to output file for storage\n",
    "with open(output_path, mode='w+') as f_out:\n",
    "    f_out.write(json_file)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "12",
   "metadata": {},
   "source": [
    "For chapter 1 General configuration, sum of tokens equals 8722.\n",
    "For chapter 2 Robot arm, sum of tokens equals 16310.\n",
    "For chapter 3 Controller, sum of tokens equals 28788.\n",
    "For chapter 4 Software, sum of tokens equals 7087.\n",
    "For chapter 5 Instruction Manual, sum of tokens equals 372.\n",
    "For chapter 6 Safety, sum of tokens equals 7411."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = \"data/\"\n",
    "filename = data_folder + \"bfp-a3447q.pdf\"\n",
    "content_path= filename.split('.')[0]+'_chunked.txt'\n",
    "context_path= filename.split('.')[0]+'_context.txt'\n",
    "image_path = \"data/images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib, json\n",
    "json_read = pathlib.Path(context_path).read_text()\n",
    "data_context = json.loads(json_read)\n",
    "data_context = {int(k): v for k, v in data_context.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 0\n",
    "for k,v in data_context.items():\n",
    "    all_tokens = tokenizer.encode(v, add_special_tokens=False)\n",
    "    if len(all_tokens) > 600:\n",
    "        if len(all_tokens) > max_len:\n",
    "            max_len = len(all_tokens)\n",
    "        print(f'For chapter no: {k}, token count is {len(all_tokens)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "json_file=json.dumps(data_context, indent=2)\n",
    "# Exporting data to output file for storage\n",
    "with open(context_path, mode='w+') as f_out:\n",
    "    f_out.write(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
