{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "## Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "! pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'snowflake/snowflake-arctic-embed-m-long'\n",
    "emb_dimensions = 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer (must match the model)\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"bfp-a3447q.pdf\"\n",
    "input_path = \"data/\" + filename.split('.')[0]+'.txt'\n",
    "output_path= input_path.split('.')[0]+'_chunked.txt'\n",
    "image_path = \"data/images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def chunk_text_by_lines(text, tokenizer, token_limit):\n",
    "    \"\"\"\n",
    "    Splits text into chunks based on token count, ensuring cuts only happen\n",
    "    between lines (preserving \\n).\n",
    "    \"\"\"\n",
    "    lines = text.splitlines(keepends=True)  # keep '\\n' at end of each line\n",
    "    chunks, current_chunk, current_tokens = [], [], 0\n",
    "    all_tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "    # setting 'soft limit'\n",
    "    chunk_size = math.ceil(len(all_tokens) / math.ceil(len(all_tokens)/ token_limit))\n",
    "    for line in lines:\n",
    "        line_tokens = tokenizer.encode(line, add_special_tokens=False)\n",
    "        if current_tokens + len(line_tokens) > token_limit:\n",
    "            # flush current chunk\n",
    "            chunks.append(\"\".join(current_chunk).strip())\n",
    "            current_chunk, current_tokens = [], 0\n",
    "        current_chunk.append(line)\n",
    "        current_tokens += len(line_tokens)\n",
    "        if current_tokens > chunk_size:\n",
    "            # flush current chunk\n",
    "            chunks.append(\"\".join(current_chunk).strip())\n",
    "            current_chunk, current_tokens = [], 0\n",
    "    \n",
    "    # add last chunk\n",
    "    if current_chunk:\n",
    "        chunks.append(\"\".join(current_chunk).strip())\n",
    "    \n",
    "    return chunks, chunk_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pathlib, json\n",
    "json_read = pathlib.Path(input_path).read_text()\n",
    "data = json.loads(json_read)\n",
    "chunked_data = []\n",
    "chunks = []\n",
    "token_limit = 1000\n",
    "print(f'Token threshold: {token_limit}')\n",
    "for index, chunk in enumerate(data):\n",
    "    text = chunk[-1]\n",
    "    # Replace <br> with semicolons or newlines to reduce token consumption and clean tables\n",
    "    text = re.sub(r\"<br\\s*/?>\", \"; \", text)\n",
    "    \n",
    "    # Strip off excessive whitespace\n",
    "    text = re.sub(r\" +\", \" \", text)\n",
    "    text = re.sub(r\"\\.+\", \".\", text)\n",
    "    text = re.sub(r\"~\", \"\", text)\n",
    "        \n",
    "    chunks, size = chunk_text_by_lines(text, tokenizer, token_limit)\n",
    "    for text in chunks:\n",
    "        chunked_data.append(\n",
    "            [chunk[0],\n",
    "             chunk[1],\n",
    "             chunk[2],\n",
    "             text]\n",
    "        )\n",
    "    if len(chunks) > 1:\n",
    "        print(f\"{index}) Text length: {len(text)}\\tToken chunk size: {size}\\tChunks count: {len(chunks)}\\t for {chunk[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "json_file=json.dumps(chunked_data, indent=2)\n",
    "# Exporting data to output file for storage\n",
    "with open(output_path, mode='w+') as f_out:\n",
    "    f_out.write(json_file)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8",
   "metadata": {},
   "source": [
    "For chapter 1 General configuration, sum of tokens equals 8722.\n",
    "For chapter 2 Robot arm, sum of tokens equals 16310.\n",
    "For chapter 3 Controller, sum of tokens equals 28788.\n",
    "For chapter 4 Software, sum of tokens equals 7087.\n",
    "For chapter 5 Instruction Manual, sum of tokens equals 372.\n",
    "For chapter 6 Safety, sum of tokens equals 7411."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = \"data/\"\n",
    "filename = data_folder + \"bfp-a3447q.pdf\"\n",
    "content_path= filename.split('.')[0]+'_chunked.txt'\n",
    "context_path= filename.split('.')[0]+'_context.txt'\n",
    "image_path = \"data/images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib, json\n",
    "json_read = pathlib.Path(context_path).read_text()\n",
    "data_context = json.loads(json_read)\n",
    "data_context = {int(k): v for k, v in data_context.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 0\n",
    "for k,v in data_context.items():\n",
    "    all_tokens = tokenizer.encode(v, add_special_tokens=False)\n",
    "    if len(all_tokens) > 600:\n",
    "        if len(all_tokens) > max_len:\n",
    "            max_len = len(all_tokens)\n",
    "        print(f'For chapter no: {k}, token count is {len(all_tokens)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "The document provides technical specifications for different robot models (RH-6CH6020-S11/S15, RH-6CH7020-S11/S15, RH-6CH6020-S23/S24, RH-6CH7020-S23/S24). These specifications include allowable inertia, J3(Z) axis pressing force, maximum eccentricity, mass, tool wiring , tool pneumatic pipes, supply pressure, protection, ambient temperature, painting color, and machine cable length.\\nNotes clarify: Note 1 covers max speed with MvTune2 and no auto compensation. Note 3 explains cycle time under MvTune2, affected by accuracy needs and carrying mass per model. Note 5 defines J3(Z) axis pressing force and overload risks. Note 8 explains setting environmental temperature with parameter OLTMX for control and protection.\\n\\nSection 2.1.2 addresses counter-force on installation surface, listing falls moment (ML), torsion moment (MT), horizontal force (FH), and vertical force (FV) for RH-3CH-Sxx, RH-6CH6020-Sxx, and RH-6CH7020-Sxx.\\n\\nSection 2.2 covers mass capacity and tooling: eccentric loads impose restrictions; tooling must follow allowable inertia values (2.1.1). Cautions include alarms (vibration, overload, overcurrent) caused by speed or posture even within limits; proper parameter settings (mass, centroid) are crucial to prevent gear/belt wear. Mass capacity and allowable inertia are dynamic limits, not guarantees of accuracy. Long or low-rigid tooling may reduce accuracy or cause vibration. Allowable offset (Z direction) from shaft to center of gravity is 100 mm. Load deviation allowance is 150 mm, reduced to 10 mm in large inertia mode. Figures 2-1 to 2-3 show center of gravity for loads.\\n\\nSection 2.2.3 explains mass capacity, speed, and acceleration/deceleration: robot optimizes automatically based on load data (mass and size). Manual reduction of speed/accel may be required if vibration or overheating occurs. Wrong load settings shorten lifespan. For high accuracy, correct load input and reduced acceleration/deceleration are recommended. Parameters: \"HNDDAT*\" (hand), \"WRKDAT*\" (work), set with \"LoadSet\" command (default LoadSet 0.0). Factory defaults for hand mass, size, and center of gravity are given in a table.\n",
    "\"\"\"\n",
    "all_tokens = tokenizer.encode(text, add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_context[34] = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "json_file=json.dumps(data_context, indent=2)\n",
    "# Exporting data to output file for storage\n",
    "with open(context_path, mode='w+') as f_out:\n",
    "    f_out.write(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
