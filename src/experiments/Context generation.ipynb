{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from pathlib import Path\n",
    "from jinja2 import Template\n",
    "import os\n",
    "import toml\n",
    "from google.genai import types\n",
    "from google import genai \n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptLoader:\n",
    "    def __init__(self, path: str = \"prompts.yaml\"):\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            self.prompts = yaml.safe_load(f)\n",
    "\n",
    "    def render(self, name: str, **kwargs) -> str:\n",
    "        \"\"\"Render a named prompt with given variables.\"\"\"\n",
    "        template = Template(self.prompts[name])\n",
    "        return template.render(**kwargs)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2",
   "metadata": {},
   "source": [
    "loader = PromptLoader(\"data/prompts.yaml\")\n",
    "prompt = loader.render(\n",
    "    \"context_extension\",\n",
    "    chunk=\">>This is the main chunk text.<<\",\n",
    "    neighbors=\">>This is the neighboring context text.<<\"\n",
    ")\n",
    "print(prompt)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3",
   "metadata": {},
   "source": [
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    config=types.GenerateContentConfig(\n",
    "        thinking_config=types.ThinkingConfig(thinking_budget=0)\n",
    "    ),\n",
    "    contents=prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"data/bfp-a3447q_chunked.txt\"\n",
    "output_path = \"data/bfp-a3447q_context.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib, json\n",
    "json_read = pathlib.Path(input_path).read_text()\n",
    "data = json.loads(json_read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph_dict = dict()\n",
    "context_dict = dict()\n",
    "loader = PromptLoader(\"data/prompts.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = toml.load(\"../../.streamlit/secrets.toml\")\n",
    "os.environ[\"GEMINI_API_KEY\"] = config[\"gemini\"][\"GOOGLE_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = genai.Client()\n",
    "print(\"Client initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_chapter_ix = 0\n",
    "window_size = 3 # max number of chunks to be taken before and ahead of selected chunk\n",
    "for index, chunk in enumerate(data):\n",
    "    if index in context_dict.keys():\n",
    "        continue\n",
    "    if chunk[0] == 1:\n",
    "        root_chapter_ix = index\n",
    "    if 2*len(chunk[1]) > len(chunk[-1]): # if chapter is empty, skip its context creation\n",
    "        print(f'{index}) Paragraphs not generated for chapter: {chunk[1]}')\n",
    "        continue\n",
    "    paragraph_start = max(root_chapter_ix, index - window_size)\n",
    "    paragraph_end = min(index + window_size + 1, len(data))\n",
    "    paragraph_list = [ d[-1] for d in data[paragraph_start:paragraph_end]]\n",
    "    paragraphs = '\\n'.join(paragraph_list)\n",
    "    print(f'{index}) Paragraphs generated in range [{paragraph_start}:{paragraph_end-1}] for chapter: {chunk[1]}')\n",
    "    paragraph_dict[index] = paragraphs\n",
    "    prompt = loader.render(\n",
    "        \"context_extension\",\n",
    "        chunk=chunk[-1],\n",
    "        neighbors=paragraphs\n",
    "    )\n",
    "    print(\"\\rPrompting standby...\",end='')\n",
    "    time.sleep(2)\n",
    "    print(\"\\rWaiting for generation...\",end='')\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.5-flash\",\n",
    "        config=types.GenerateContentConfig(\n",
    "            thinking_config=types.ThinkingConfig(thinking_budget=0)\n",
    "        ),\n",
    "        contents=prompt\n",
    "    )\n",
    "    print(f'\\r\\tContext generated with end: {response.text[-50:]}')\n",
    "    context_dict[index] = response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "json_file=json.dumps(context_dict, indent=2)\n",
    "# Exporting data to output file for storage\n",
    "with open(output_path, mode='w+') as f_out:\n",
    "    f_out.write(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
